{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,-0.36603121543,0.0225539737063,0.0120457731673]\n",
      "Intercept: -0.43834917889\n",
      "numIterations: 11\n",
      "objectiveHistory: [0.4999999999999999, 0.4850034222802328, 0.45148643215923123, 0.45096736920348357, 0.45069319592989965, 0.4506884629300494, 0.450688302072468, 0.4506882541560428, 0.45068825023326803, 0.45068824911749356, 0.45068824892020054]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-0.5570555069166132|\n",
      "|-0.4417654113732035|\n",
      "|-0.8856666380752383|\n",
      "|0.02411746709945639|\n",
      "|  0.723482753634509|\n",
      "| 1.1368873356310882|\n",
      "+-------------------+\n",
      "\n",
      "RMSE: 0.719536\n",
      "r2: 0.357298\n",
      "+-----+-------------------+---+---------+--------------+------+------------------+\n",
      "|label|           features|age|workclass|hours_per_week|income|        prediction|\n",
      "+-----+-------------------+---+---------+--------------+------+------------------+\n",
      "|  1.0|[1.0,0.0,37.0,50.0]| 37|  Private|            50|  =50K|0.9984365066068699|\n",
      "+-----+-------------------+---+---------+--------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from operator import add\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"LinearRegressionWithElasticNet\") \\\n",
    "    .getOrCreate()\n",
    "\t\n",
    "#creating dataframe\t\n",
    "ad_data= spark\\\n",
    ".read\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"adult6.csv\")\n",
    "ad_data.createOrReplaceTempView(\"adult\")\n",
    "dataset = spark.table(\"adult\")\n",
    "cols = dataset.columns\n",
    "#print cols\n",
    "\n",
    "\n",
    "categoricalColumns = [\"workclass\"]\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "\tstringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "\t#In the above line for example, it takes workclass string and concatinates with the address(\"Index\")\n",
    "\tencoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "\tstages += [stringIndexer, encoder]\n",
    "#print stages\n",
    "\n",
    "\n",
    "#\n",
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"income\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"age\",\"hours_per_week\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "#print stages\n",
    "#print assembler\n",
    "\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(dataset)\n",
    "dataset = pipelineModel.transform(dataset)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "\n",
    "# we can use print dataset\n",
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "# $example off$\n",
    "dataset = lrModel.transform(testData)\n",
    "dataset.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
