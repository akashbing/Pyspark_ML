{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.19184160783,-10.220627656,0.316523376544,-0.170779945632]\n",
      "+-----+----------+------------------------------------------+------+\n",
      "|label|prediction|probability                               |income|\n",
      "+-----+----------+------------------------------------------+------+\n",
      "|1.0  |0.0       |[0.9996270840069971,3.7291599300297534E-4]| >50K |\n",
      "+-----+----------+------------------------------------------+------+\n",
      "\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from operator import add\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Logistic regression with binomial\") \\\n",
    "    .getOrCreate()\n",
    "\t\n",
    "#creating dataframe\t\n",
    "ad_data= spark\\\n",
    ".read\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"/Users/akashsoni/adult5.csv\")\n",
    "ad_data.createOrReplaceTempView(\"adult\")\n",
    "dataset = spark.table(\"adult\")\n",
    "cols = dataset.columns\n",
    "#print cols\n",
    "\n",
    "\n",
    "#categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "categoricalColumns = [\"workclass\"]\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "\tstringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "\t#.fit(ad_data)\n",
    "\t#df_numeric = stringIndexer.transform(ad_data)\n",
    "\t#df_numeric.repartition(1).repartition(1).write.csv('indexer')\n",
    "\t#print df_numeric.select('workclass','workclassIndex').show(5)\n",
    "\t#In the above line for example, it takes workclass string and concatinates with the address(\"Index\")\n",
    "\tencoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "\t#print encoder.outputCol\n",
    "\tstages += [stringIndexer, encoder]\n",
    "#print stages\n",
    "\n",
    "\n",
    "#\n",
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"income\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "#numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "numericCols = [\"age\",\"hours_per_week\"]\n",
    "assemblerInputs = list(map(lambda c: c + \"classVec\", categoricalColumns)) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "#print stages\n",
    "#print assembler\n",
    "\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(dataset)\n",
    "dataset = pipelineModel.transform(dataset)\n",
    "#dataset.printSchema()\n",
    "#dataset.repartition(1).write.format(\"json\").save(\"c:\\\\spark\\\\ml_detail\\\\step1_transform_features\")\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "\n",
    "#dataset.printSchema()\n",
    "#dataset.show(5)\n",
    "\n",
    "\n",
    "# we can use print dataset\n",
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "#trainnig data\n",
    "#trainingData.repartition(1).write.format(\"json\").save(\"c:\\\\spark\\\\ml_detail\\\\step2_training_data\")\n",
    "#training data\n",
    "#testData.repartition(1).write.format(\"json\").save(\"c:\\\\spark\\\\ml_detail\\\\step3_test_data\")\n",
    "#print trainingData.count()\n",
    "#print testData.count()\n",
    "\n",
    "\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10,family=\"binomial\")\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "######################### difference between multiclass/binary is coefficients ########################\n",
    "print (lrModel.coefficients)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "#predictions.printSchema()\n",
    "\n",
    "# View model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well. For example's sake we will choose income & occupation\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"income\")\n",
    "#selected.printSchema()\n",
    "selected.show(truncate=False)\n",
    "\n",
    "#selected.repartition(1).write.format(\"json\").save(\"logistics_data\")\n",
    "\n",
    "\n",
    "#binary classification\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "print (evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator.getMetricName()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
