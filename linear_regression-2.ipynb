{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_41bea0f9812ceca3fca9, OneHotEncoder_46349e0e749145854b84]\n",
      "[StringIndexer_41bea0f9812ceca3fca9, OneHotEncoder_46349e0e749145854b84, StringIndexer_4e2facce7b6750e5e678, VectorAssembler_461c8714299b1b29326f]\n",
      "VectorAssembler_461c8714299b1b29326f\n",
      "Coefficients: [0.0,0.0,0.010425881948,0.0]\n",
      "Intercept: -0.1305446911035797\n",
      "numIterations: 11\n",
      "objectiveHistory: [0.5, 0.49248840179791875, 0.46543968467416663, 0.46337810593100537, 0.46303177080326474, 0.4629769269117229, 0.4629682421084831, 0.46296686682665666, 0.46296664904393336, 0.4629666145569551, 0.46296660909577025]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "| -0.2760647048701082|\n",
      "|-0.10925059370141574|\n",
      "|-0.20308353123380524|\n",
      "|  0.5883988298053292|\n",
      "+--------------------+\n",
      "\n",
      "RMSE: 0.344820\n",
      "r2: 0.365862\n",
      "+-----+-------------------+---+---------+------+---------+-------------+---------------+-------+--------------+------+-------------------+\n",
      "|label|           features|age|workclass|fnlwgt|education|education_num|     occupation|    sex|hours_per_week|income|         prediction|\n",
      "+-----+-------------------+---+---------+------+---------+-------------+---------------+-------+--------------+------+-------------------+\n",
      "|  1.0|[1.0,0.0,31.0,50.0]| 31|  Private| 45781|  Masters|           14| Prof-specialty| Female|            50|  >50K|0.19265764928576196|\n",
      "+-----+-------------------+---+---------+------+---------+-------------+---------------+-------+--------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"LinearRegressionWithElasticNet\") \\\n",
    "    .getOrCreate()\n",
    "\t\n",
    "#creating dataframe\t\n",
    "ad_data= spark\\\n",
    ".read\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"adult_lr_5.csv\")\n",
    "ad_data.createOrReplaceTempView(\"adult\")\n",
    "dataset = spark.table(\"adult\")\n",
    "cols = dataset.columns\n",
    "#print cols\n",
    "\n",
    "####### if you would like to check how the dataframe looks like and it's columns ######\n",
    "\n",
    "#ad_data.createOrReplaceTempView(\"adult\")\n",
    "#dataset = spark.table(\"adult\")\n",
    "#cols = dataset.columns\n",
    "#print cols\n",
    "\n",
    "############# Columns ##################\n",
    "\n",
    "categoricalColumns = [\"workclass\"]\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "\tstringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "\t#In the above line for example, it takes workclass string and concatinates with the address(\"Index\")\n",
    "\tencoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "    # onehotencoder will take n-1 distinct values and convert to vector\n",
    "\tstages += [stringIndexer, encoder]\n",
    "print (stages)\n",
    "\n",
    "\n",
    "# Convert label into label indices using the StringIndexer\n",
    "# means in our example we have <50k and >=50k so <50k will get label 0.0 and >50k will get label 1.0\n",
    "label_stringIdx = StringIndexer(inputCol = \"income\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"age\",\"hours_per_week\"]\n",
    "assemblerInputs = list(map(lambda c: c + \"classVec\", categoricalColumns)) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "print (stages)\n",
    "print (assembler)\n",
    "\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(dataset)\n",
    "dataset = pipelineModel.transform(dataset)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "\n",
    "# we can use print dataset\n",
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "### so if we have 100 records then 70 will be in training and 30 will be in testing (approximately)\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "# $example off$\n",
    "dataset = lrModel.transform(testData)\n",
    "dataset.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
